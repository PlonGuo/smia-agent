# SmIA Progress Tracker — AI Daily Digest Feature
# Archived: 2026-02-25
# Status: Backend complete, frontend pending

## Feature Overview
AI Daily Intelligence Aggregator — daily digest from arXiv, GitHub Trending, RSS/Blogs, and Bluesky.
Plan: docs/plans/2026-02-24-ai-daily-digest.md

## Implementation Steps

### Step 1: Schema & Models (DONE)
- [x] Step 1.1: Create migration SQL (002_add_digest_tables.sql)
- [x] Step 1.2: Run migration via Supabase MCP (8 tables + RLS + RPC functions created)
- [x] Step 1.3: Enable Supabase Realtime (daily_digests, digest_access_requests, user_bindings)
- [x] Step 1.4: Create Pydantic models (digest_schemas.py)
- [x] Step 1.5: Add TypeScript types (shared/types.ts)
- [x] Step 1.6: Seed admin (jason.ghzr@gmail.com)
- [x] Step 1.7: Write model unit tests (25 tests passing)

### Step 2: Permissions & Email (DONE)
- [x] Step 2.1: Update config (resend_api_key, admin_email, bluesky_app_password, internal_secret, app_url)
- [x] Step 2.2: Create email service (Resend API)
- [x] Step 2.3: Add permission helpers (is_admin, get_digest_access_status, seed_admin_if_empty)
- [x] Step 2.4: Create admin routes + register in index.py + lifespan for admin seeding
- [x] Step 2.5: Write tests (13 tests passing)

### Step 3: Collectors (DONE)
- [x] Step 3.1: Install dependencies (arxiv, feedparser)
- [x] Step 3.2: Create Collector Protocol + Registry (base.py)
- [x] Step 3.3: Create 4 collectors (arXiv, GitHub, RSS, Bluesky) + rss_feeds.json config
- [x] Step 3.4: Self-registration in __init__.py
- [x] Step 3.5: Write collector unit tests (22 tests passing)

### Step 4: Digest Agent (DONE)
- [x] Step 4.1: Create digest agent (digest_agent.py — GPT-4.1, structured output)
- [x] Step 4.2: Write agent tests

### Step 5: Two-Phase Orchestrator (DONE)
- [x] Step 5.1: Create two-phase orchestrator (digest_service.py — collectors + LLM phases)
- [x] Step 5.2: Add Telegram notification (notify_digest_ready with summary)
- [x] Step 5.3: Write orchestrator tests

### Step 6: API Routes (DONE)
- [x] Step 6.1: Create report routes (ai_daily_report.py — /today, /status, /list, /{id})
- [x] Step 6.2: Create share token + access request endpoints
- [x] Step 6.3: Register routers + admin seeding in index.py
- [x] Step 6.4: Internal endpoints (/internal/collect, /internal/analyze)
- [x] Step 6.5: Write route tests

### Step 7-9: Frontend (PENDING)
- [ ] Step 7: Frontend hooks, admin page, permissions UI
- [ ] Step 8: AiDailyReport page (4-state: loading, generating, completed, error)
- [ ] Step 9: History, Sharing, Export pages

### Step 11: Integration & Polish (PENDING)
- [ ] Live smoke tests
- [ ] Integration tests + polish

## Production Bug Fixes (post-deploy)
- [x] Fix: Telegram /digest stuck — collectors run inline instead of BackgroundTask (Vercel limitation)
- [x] Fix: Staleness recovery — reset digests stuck >5 min in collecting/analyzing
- [x] Fix: Conditional ordering in handle_digest() — check "claimed" before "status"
- [x] Fix: Include executive_summary in Telegram digest notification

## Architecture Decision: Two-Phase Pipeline
Split collectors and LLM into separate serverless functions to stay within Vercel Hobby 60s limit:
- Phase 1 (Function A): Run 4 collectors → save to cache → HTTP trigger Phase 2
- Phase 2 (Function B): Read cached data → LLM analysis → save completed digest
Each phase gets its own 60s budget. Internal endpoint secured with shared secret.

## Key Decisions
- Two-phase pipeline split for Vercel Hobby compatibility
- GPT-4.1 for digest analysis (~$0.12/day)
- Lazy trigger (no cron) — first authorized user visit generates digest
- PostgreSQL atomic RPC for race condition handling
- Telegram runs collectors inline (BackgroundTasks unreliable on Vercel)
